{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:05:31.688869700Z",
     "start_time": "2024-01-25T04:05:21.777166200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run this on HPC\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import neuralforecast\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn\n",
    "from optuna.trial import TrialState\n",
    "from optuna.samplers import TPESampler\n",
    "from neuralforecast.auto import AutoTCN\n",
    "from neuralforecast.models import TCN\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import HuberLoss\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import csv\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "\n",
    "#Show all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc768e91d893729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:05:31.731003Z",
     "start_time": "2024-01-25T04:05:31.698864400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Change directory to parent directory if necessary\n",
    "if os.getcwd() == '/home/USACE_Modeling':\n",
    "    None\n",
    "else:\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "par = os.getcwd() #Parent Directory\n",
    "par = Path(par)\n",
    "sys.path.append(str(par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545fa27a1668624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:05:31.749154700Z",
     "start_time": "2024-01-25T04:05:31.735095Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#List of data files\n",
    "data_model_dict = {1:'UARK_WCS_AIS_Compiled_NewData_No_Aggregation.csv', 2:'UARK_WCS_AIS_Compiled_NewData_Mixed.csv', 3:'UARK_WCS_AIS_Compiled_NewData_Self-Propelled, Dry Cargo.csv',4:'UARK_WCS_AIS_Compiled_NewData_Self-Propelled, Tanker.csv',5:'UARK_WCS_AIS_Compiled_NewData_Tug_Tow.csv'}\n",
    "\n",
    "data_model_dict = {1:'UARK_WCS_AIS_Compiled_NewData_No_Aggregation.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b4cfb49bc90fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:05:39.401004400Z",
     "start_time": "2024-01-25T04:05:39.384694100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(pl.Callback, pl.Trainer):\n",
    "    def __init__(self, save_path):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "        self.save_path = save_path  # Path to save the CSV file\n",
    "        self.epoch = []\n",
    "        self.a_trn_loss = []\n",
    "        self.a_val_loss = []\n",
    "        self.a_val_MAE = {}\n",
    "        self.a_val_RMSE = {}\n",
    "\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        self.a_trn_loss = np.ones(trainer.max_epochs) * np.inf\n",
    "        self.a_val_loss = np.ones(trainer.max_epochs) * np.inf\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.epoch = np.append(self.epoch, trainer.current_epoch)        \n",
    "        self.a_trn_loss.append(trainer.callback_metrics[\"train_loss\"].item())\n",
    "        self.a_val_loss.append(trainer.callback_metrics[\"valid_loss\"].item())\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        self.save_metrics_to_csv()\n",
    "\n",
    "    def save_metrics_to_csv(self):\n",
    "        csv_save_path = self.save_path + '/metrics.csv'\n",
    "        with open(csv_save_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"epochs\", \"train_loss\", \"valid_loss\"])\n",
    "            for epoch in range(len(self.a_val_loss)):\n",
    "                writer.writerow([epoch, self.a_trn_loss[epoch], self.a_val_loss[epoch]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349fc70da86c139e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:05:43.708800600Z",
     "start_time": "2024-01-25T04:05:41.147865700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_combined_list = []\n",
    "prediction_combined_df = pd.DataFrame()\n",
    "for i_filenumber in data_model_dict.keys():\n",
    "    data_iden = i_filenumber\n",
    "        \n",
    "    study_name = data_model_dict[data_iden].replace('.csv','')\n",
    "    \n",
    "    def dataset(data_model_dir = data_model_dict,  data_num=data_iden, par_dir=par):    \n",
    "    \n",
    "        #Show all columns in dataframe\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        begin_testing = '2020Q1'\n",
    "        end_testing = '2020Q4'\n",
    "        \n",
    "        batch_size = 128  # set this between 32 to 128\n",
    "        #Read Main Data\n",
    "        wcs_df = pd.read_csv(par_dir / 'Data' / 'UARK_WCS_AIS_Compiled_NewData.csv')\n",
    "        cons_loc = pd.read_csv(par_dir / 'Data' / 'location_with_consistent_data_newdata.csv')\n",
    "        wcs_df = pd.merge(wcs_df,cons_loc,how='left',on='sub_folder')\n",
    "        port_terminal = wcs_df[['sub_folder', 'folder']].drop_duplicates()\n",
    "        \n",
    "        #Read Data\n",
    "        file_loc = par_dir / 'Data' / data_model_dir[data_num]\n",
    "        df = pd.read_csv(file_loc)\n",
    "        #Drop columns that start with 'dwell\n",
    "        df = df.drop(columns=[col for col in df.columns if col.lower().startswith('dwell_'.lower())])\n",
    "        \n",
    "        df = df[df[\"quarter\"] <= end_testing]\n",
    "        df = pd.merge(df, port_terminal, on=\"sub_folder\", how=\"left\")\n",
    "        df[\"ds\"] = pd.PeriodIndex(df[\"quarter\"], freq=\"Q\").to_timestamp()\n",
    "        # Rename some columns\n",
    "        df.rename(columns={\"sub_folder\": \"terminal\", \"QuarterOfTheYear\": \"quarter_of_year\", \"folder\": \"port\"}, inplace=True)\n",
    "        targets = [f\"C_{i}\" for i in range(1, 10)]\n",
    "        ais_features = [col for col in df.columns if col.startswith(\"stop_count\") or col.startswith(\"dwell_per_stop\")]\n",
    "        # Melt the DataFrame 'df' to a long format\n",
    "        data = pd.melt(\n",
    "            df,\n",
    "            id_vars=[\n",
    "                \"terminal\",\n",
    "                \"port\",\n",
    "                \"ds\",\n",
    "                \"quarter\",\n",
    "            ]\n",
    "            + ais_features,\n",
    "            value_vars=targets,\n",
    "            var_name=\"commodity\",\n",
    "            value_name=\"y\",\n",
    "        )\n",
    "        \n",
    "        # Create a new column 'key' that combines the values in the 'port', 'terminal', and 'commodity' columns\n",
    "        data[\"unique_id\"] = data[\"port\"].astype(str) + \"|\" + data[\"terminal\"].astype(str) + \"|\" + data[\"commodity\"].astype(str)\n",
    "        #Removing outliers\n",
    "        outlier_terminals = pd.read_csv(par_dir / 'Data' / 'outlier_terminals.csv')\n",
    "        outlier_terminals_commodity = pd.read_csv(par_dir / 'Data' / 'outlier_terminals_commodity.csv')\n",
    "        #Remove records from data where terminals are in outlier_terminals\n",
    "        data = data[~data['terminal'].isin(outlier_terminals['terminal'])]\n",
    "        #Remove records from data where key is in outlier_terminals_commodity\n",
    "        data = data[~data['unique_id'].isin(outlier_terminals_commodity['unique_id'])]\n",
    "        \n",
    "        data[\"year\"] = data[\"ds\"].dt.year\n",
    "        # Create four binary columns (Q1, Q2, Q3, Q4) based on the \"ds\" column\n",
    "        data[\"Q1\"] = (data[\"ds\"].dt.quarter == 1).astype(int)\n",
    "        data[\"Q2\"] = (data[\"ds\"].dt.quarter == 2).astype(int)\n",
    "        data[\"Q3\"] = (data[\"ds\"].dt.quarter == 3).astype(int)\n",
    "        data[\"Q4\"] = (data[\"ds\"].dt.quarter == 4).astype(int)\n",
    "        \n",
    "        fut_exog_features = ais_features + [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "        past_exog_features = fut_exog_features + [\"y\"]\n",
    "        \n",
    "        data[\"unique_id\"] = data[\"port\"].astype(str) + \"|\" + data[\"terminal\"].astype(str) + \"|\" + data[\"commodity\"].astype(str)\n",
    "        data[\"year\"] = data[\"ds\"].dt.year\n",
    "        \n",
    "        #Drop columns terminal, port, quarter, commodity\n",
    "        data.drop(columns=[\"terminal\", \"port\", \"commodity\", \"year\"], inplace=True)\n",
    "        \n",
    "        train_df, test_df = data[data[\"quarter\"] < begin_testing], data[data[\"quarter\"] >= begin_testing]\n",
    "        \n",
    "        #Drop quarter column\n",
    "        train_df.drop(columns=[\"quarter\"], inplace=True)\n",
    "        test_df.drop(columns=[\"quarter\"], inplace=True)\n",
    "        \n",
    "        \n",
    "        return dict(\n",
    "            train_df=train_df,\n",
    "            test_df=test_df,\n",
    "            fut_exog_features=fut_exog_features,\n",
    "            past_exog_features=past_exog_features,\n",
    "        )\n",
    "    dataset_return = dataset()\n",
    "    train_df = dataset_return['train_df']\n",
    "    test_df = dataset_return['test_df']\n",
    "    fut_exog_features = dataset_return['fut_exog_features']\n",
    "    past_exog_features = dataset_return['past_exog_features']\n",
    "    \n",
    "    #Get the folder name that starts with prediction_folder\n",
    "    result_folder = par / 'Outputs' / 'TCN_Outputs' \n",
    "    folders = [f for f in os.listdir(result_folder) if os.path.isdir(os.path.join(result_folder, f))]\n",
    "    best_trial_folder = [f for f in folders if f.startswith(study_name)][0]\n",
    "    \n",
    "    best_trial_path = par / 'Outputs' / 'TCN_Outputs' / best_trial_folder\n",
    "    \n",
    "    print('Best Trial is ' + str(best_trial_folder))\n",
    "    \n",
    "    #Load the best model\n",
    "    load_tcn = NeuralForecast.load(path=best_trial_path, verbose=True)\n",
    "    predictions = load_tcn.predict(futr_df=test_df, verbose=True).reset_index()\n",
    "    results_df = pd.merge(predictions, test_df[[\"ds\", \"unique_id\", \"y\"]], on=[\"ds\", \"unique_id\"], how=\"left\")\n",
    "    #Rename y to Actuals, TCN to Predictions\n",
    "    results_df.rename(columns={\"y\": \"Actuals\", \"TCN\": \"Predictions\"}, inplace=True)\n",
    "    #Break unique_id into PSA, sub_folder, Commodity\n",
    "    results_df[[\"PSA\", \"sub_folder\", \"Commodity\"]] = results_df[\"unique_id\"].str.split(\"|\", expand=True)\n",
    "    #Add column called Model\n",
    "    results_df[\"Model\"] = \"TCN\"\n",
    "    #Add column Aggregation\n",
    "    results_df[\"Aggregation\"] = study_name.split('NewData_')[1]\n",
    "    #Convert ds to quarter\n",
    "    results_df[\"quarter\"] = pd.PeriodIndex(results_df[\"ds\"], freq=\"Q\")\n",
    "    #Drop ds\n",
    "    results_df.drop(columns=[\"ds\", \"unique_id\", \"PSA\"], inplace=True)\n",
    "    \n",
    "    #Append to prediction_combined_list\n",
    "    prediction_combined_list.append(results_df)\n",
    "    \n",
    "#Concatenate all dataframes in prediction_combined_list\n",
    "prediction_combined_df = pd.concat(prediction_combined_list)\n",
    "print(prediction_combined_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b599348592e74",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Export to csv. Change mode to 'w' if you want to overwrite\n",
    "####prediction_combined_df.to_csv(par / 'Outputs' / 'TCN_Outputs' / 'TCN_Predictions.csv', index=False, mode='x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
