{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:06:41.757107Z",
     "start_time": "2024-01-25T04:06:34.709153100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "#Seeds for reproducibility\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import csv\n",
    "import lightning.pytorch as pl\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer, MultiNormalizer\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, MAPE, PoissonLoss, QuantileLoss\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.strategies import FSDPStrategy\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "from optuna.study import MaxTrialsCallback\n",
    "from optuna.trial import TrialState\n",
    "import gc\n",
    "#Change directory to parent directory if necessary\n",
    "if os.getcwd() == '/home/USACE_Modeling':\n",
    "    None\n",
    "else:\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "par = os.getcwd() #Parent Directory\n",
    "par = Path(par)\n",
    "sys.path.append(str(par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284f6e4dbe20c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:07:12.253491900Z",
     "start_time": "2024-01-25T04:06:55.437381800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_folder = 'Result_3'\n",
    "#List of data files\n",
    "data_model_dict = {1:'UARK_WCS_AIS_Compiled_NewData_No_Aggregation.csv', 2:'UARK_WCS_AIS_Compiled_NewData_Mixed.csv', 3:'UARK_WCS_AIS_Compiled_NewData_Self-Propelled, Dry Cargo.csv',4:'UARK_WCS_AIS_Compiled_NewData_Self-Propelled, Tanker.csv',5:'UARK_WCS_AIS_Compiled_NewData_Tug_Tow.csv'}\n",
    "\n",
    "data_model_dict = {1:'UARK_WCS_AIS_Compiled_NewData_No_Aggregation.csv'}\n",
    "\n",
    "prediction_combined_list = []\n",
    "prediction_combined_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i_filenumber in data_model_dict.keys():    \n",
    "    if os.getcwd() == '/home/USACE_Modeling':\n",
    "        data_iden = int(sys.argv[1]) #For HPC\n",
    "    else:\n",
    "        data_iden=i_filenumber\n",
    "            \n",
    "    study_name = data_model_dict[data_iden].replace('.csv','')\n",
    "    def dataset(data_model_dir = data_model_dict,  data_num=data_iden, par_dir=par):    \n",
    "    \n",
    "        #Show all columns in dataframe\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        begin_testing = '2020Q1'\n",
    "        end_testing = '2020Q4'\n",
    "        \n",
    "        batch_size = 32  # set this between 32 to 128\n",
    "        #Read Main Data\n",
    "        wcs_df = pd.read_csv(par_dir / 'Data' / 'UARK_WCS_AIS_Compiled_NewData.csv')\n",
    "        cons_loc = pd.read_csv(par_dir / 'Data' / 'location_with_consistent_data_newdata.csv')\n",
    "        wcs_df = pd.merge(wcs_df,cons_loc,how='left',on='sub_folder')\n",
    "        port_terminal = wcs_df[['sub_folder', 'folder']].drop_duplicates()\n",
    "        \n",
    "        #Read Data\n",
    "        file_loc = par_dir / 'Data' / data_model_dir[data_num]\n",
    "        df = pd.read_csv(file_loc)\n",
    "        #Drop columns that start with 'dwell\n",
    "        df = df.drop(columns=[col for col in df.columns if col.lower().startswith('dwell_'.lower())])\n",
    "        \n",
    "        df = df[df[\"quarter\"] <= end_testing]\n",
    "        df = pd.merge(df, port_terminal, on=\"sub_folder\", how=\"left\")\n",
    "        # Split the values in the 'quarter' column by \"Q\" and convert them to integers\n",
    "        temp = df[\"quarter\"].str.split(\"Q\", expand=True).astype(int)\n",
    "        # Calculate the number of quarters from the start of the data and add it as a new column 'quarter_from_start'\n",
    "        df[\"year\"] = temp[0]\n",
    "        df[\"time_idx\"] = (temp[0] - temp[0].iloc[0]) * 4 + temp[1] - 1\n",
    "        df.rename(columns={\"sub_folder\": \"terminal\", \"QuarterOfTheYear\": \"quarter_of_year\", \"folder\": \"port\"}, inplace=True)\n",
    "        target = [col for col in df.columns if col.startswith('C_')]\n",
    "        ais_features = [col for col in df.columns if col.startswith(\"stop_count\") or col.startswith(\"dwell_per_stop\")]\n",
    "        # Melt the DataFrame 'df' to a long format\n",
    "        data = pd.melt(\n",
    "            df,\n",
    "            id_vars=[\n",
    "                \"terminal\",\n",
    "                \"port\",\n",
    "                \"quarter_of_year\",\n",
    "                \"quarter\",\n",
    "                \"year\",\n",
    "                \"time_idx\",\n",
    "            ]\n",
    "            + ais_features,\n",
    "            value_vars=target,\n",
    "            var_name=\"commodity\",\n",
    "            value_name=\"volume\",\n",
    "        )\n",
    "        # Create a new column 'key' that combines the values in the 'port', 'terminal', and 'commodity' columns\n",
    "        data[\"key\"] = data[\"port\"].astype(str) + \"|\" + data[\"terminal\"].astype(str) + \"|\" + data[\"commodity\"].astype(str)\n",
    "        \n",
    "        outlier_terminals = pd.read_csv(par_dir / 'Data' / 'outlier_terminals.csv')\n",
    "        outlier_terminals_commodity = pd.read_csv(par_dir / 'Data' / 'outlier_terminals_commodity.csv')\n",
    "        \n",
    "        #Remove records from data where terminal is in outlier_terminals\n",
    "        data = data[~data['terminal'].isin(outlier_terminals['terminal'])]\n",
    "        #Remove records from data where key is in outlier_terminals_commodity\n",
    "        data = data[~data['key'].isin(outlier_terminals_commodity['key'])]\n",
    "        \n",
    "        #Drop port, terminal, and commodity columns\n",
    "        data = data.drop(columns=[\"port\", \"terminal\", \"commodity\"])\n",
    "        #Set quarter of year as string\n",
    "        data['quarter_of_year'] = data['quarter_of_year'].astype(str)\n",
    "        #Split into train and test\n",
    "        train_df = data[data['quarter'] < begin_testing]\n",
    "        test_df = data[data['quarter'] >= begin_testing]\n",
    "        #Drop quarter\n",
    "        train_df = train_df.drop(columns=['quarter'])\n",
    "        test_df = test_df.drop(columns=['quarter'])\n",
    "        max_prediction_length = 4\n",
    "        max_encoder_length = 4\n",
    "        training_cutoff = train_df[\"time_idx\"].max() - max_prediction_length\n",
    "            \n",
    "        # Create training dataset\n",
    "        training_ret = TimeSeriesDataSet(\n",
    "            train_df[lambda x: x.time_idx <= training_cutoff],\n",
    "            time_idx=\"time_idx\",\n",
    "            target=\"volume\",\n",
    "            group_ids=[\"key\"],\n",
    "            min_encoder_length=max_encoder_length,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            min_prediction_length=max_prediction_length,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"key\"],\n",
    "            time_varying_known_categoricals=[\"quarter_of_year\"],\n",
    "            time_varying_known_reals=ais_features + [\"time_idx\"],\n",
    "            time_varying_unknown_reals=[\"volume\"],\n",
    "            target_normalizer=GroupNormalizer(groups=[\"key\"], transformation=\"relu\"),\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "        )\n",
    "        \n",
    "        # create validation set (predict=True) which means to predict the last max_prediction_length points in time for each series\n",
    "        validation_ret = TimeSeriesDataSet.from_dataset(training_ret, train_df, predict=True, stop_randomization=True)\n",
    "        \n",
    "        # create dataloaders for model\n",
    "        train_dataloader_ret = training_ret.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "        val_dataloader_ret = validation_ret.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "        \n",
    "        \n",
    "        #Create dictionry to return\n",
    "        return_dict = {'train_dataloader':train_dataloader_ret, 'val_dataloader':val_dataloader_ret, 'training':training_ret, 'test':test_df, 'train':train_df}\n",
    "        return return_dict\n",
    "\n",
    "    \n",
    "    #Get values from dataset\n",
    "    data_dict = dataset()\n",
    "    train_dataloader = data_dict['train_dataloader']\n",
    "    val_dataloader = data_dict['val_dataloader']\n",
    "    training = data_dict['training']\n",
    "    train_set = data_dict['train']\n",
    "    test_set = data_dict['test']\n",
    "\n",
    "    \n",
    "    #Clear memory\n",
    "    try:\n",
    "        print(\"Clearing data_dict\", flush=True)\n",
    "        del data_dict\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        None\n",
    "    # ### Load Best Model\n",
    "    #Find file name that starts with study_name and ends with .ckpt\n",
    "    for file in os.listdir(par / 'Outputs' / 'TFT_Outputs' / prediction_folder):\n",
    "        if file.startswith(study_name) and file.endswith('.ckpt'):\n",
    "            file_name = file\n",
    "            \n",
    "    print(file_name)\n",
    "    \n",
    "    tft = TemporalFusionTransformer.load_from_checkpoint(par / 'Outputs' / 'TFT_Outputs' / prediction_folder / file_name)\n",
    "    \n",
    "    #Stack test df at the end of train df\n",
    "    test = pd.concat([train_set, test_set], ignore_index=True).fillna(0)\n",
    "    \n",
    "    #Create test dataset\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(training, test, predict=True, stop_randomization=True)\n",
    "    #Create dataloader for test dataset\n",
    "    test_dataloader = test_dataset.to_dataloader(train=False, batch_size=32, num_workers=0)\n",
    "    actuals = torch.cat([y[0] for x, y in iter(test_dataloader)])\n",
    "    actual_values = actuals.tolist()\n",
    "    #Get prediction resutls\n",
    "    prediction  = tft.predict(test_dataloader, return_index=True)\n",
    "    prediction_values = prediction.output.tolist()\n",
    "    index_keys = prediction.index.key.tolist()\n",
    "    prediction_df = pd.DataFrame({\"key\": index_keys, \"Actuals\": actual_values, \"Predictions\": prediction_values})\n",
    "    prediction_df[\"quarter\"] = [[\"2020Q1\", \"2020Q2\", \"2020Q3\", \"2020Q4\"]] * prediction_df.shape[0]\n",
    "    prediction_df[[\"folder\", \"sub_folder\", \"Commodity\"]] = prediction_df[\"key\"].str.split(\"|\", expand=True)\n",
    "    prediction_df.drop(columns=[\"key\"], inplace=True)\n",
    "    prediction_df = prediction_df.explode([\"Actuals\", \"Predictions\", \"quarter\"]).reset_index(drop=True)\n",
    "    prediction_df['Model'] = 'TFT'\n",
    "    prediction_df['Aggregation'] = study_name.split('NewData_')[1]\n",
    "    #If quarter>=begin_testing, then prediction_df['Set'] = Testing\n",
    "    # prediction_df['Set'] = np.where(prediction_df['quarter']>='2020Q1', 'Testing', 'Training')\n",
    "    #Convert Actuals and Predictions to int\n",
    "    prediction_df['Actuals'] = prediction_df['Actuals'].astype(int)\n",
    "    prediction_df['Predictions'] = prediction_df['Predictions'].astype(int)\n",
    "    #Drop folder\n",
    "    prediction_df = prediction_df.drop(columns=['folder'])\n",
    "    #Append to list\n",
    "    prediction_combined_list.append(prediction_df)\n",
    "\n",
    "prediction_combined_df = pd.concat(prediction_combined_list)\n",
    "prediction_combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67403191738f68d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:36:44.550561700Z",
     "start_time": "2023-11-20T22:36:44.511527100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Export to csv in Outputs folder. Change mode to 'w' if you want to overwrite\n",
    "###prediction_combined_df.to_csv(par / 'Outputs' / 'TFT_Outputs' / prediction_folder / 'TFT_Predictions.csv', index=False, mode='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d043ef8ed28474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:07:16.619318400Z",
     "start_time": "2024-01-25T04:07:15.382186700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Print model hyperparameters\n",
    "storage_path = \"Outputs/TFT_Outputs/\" + prediction_folder + \"/TFT_study.log\"\n",
    "storage_url = optuna.storages.JournalStorage(optuna.storages.JournalFileStorage(storage_path),)\n",
    "study = optuna.study.load_study(study_name = study_name, storage=storage_url)\n",
    "study_temp_df = study.trials_dataframe()\n",
    "#Get best trial\n",
    "best_trial = study_temp_df[study_temp_df['value']==study_temp_df['value'].min()]\n",
    "\n",
    "#Keep row with first lowest value from best_trial\n",
    "best_hp = best_trial[best_trial['value']==best_trial['value'].min()].iloc[0]\n",
    "#Print best hyperparameters\n",
    "print('Best hyperparameters:')\n",
    "for key, value in best_hp.to_dict().items():\n",
    "    print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491b073df863885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T04:17:15.187867900Z",
     "start_time": "2024-01-25T04:17:10.999185Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "raw_predictions = tft.predict(test_dataloader, mode=\"raw\", return_index=True)\n",
    "interpretation = tft.interpret_output(raw_predictions.output, reduction=\"sum\")\n",
    "tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723b16b13813de7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
