{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:13:50.118190200Z",
     "start_time": "2024-02-12T18:13:24.764338900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import os\n",
    "#Change directory to parent directory if necessary\n",
    "if os.getcwd() == '/home/USACE_Modeling':\n",
    "    None\n",
    "else:\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "    \n",
    "import sys\n",
    "import ast\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #Use CPU\n",
    "\n",
    "import tensorflow as tf\n",
    "rn.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import Huber, MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "import optuna.storages\n",
    "from optuna.storages import RetryFailedTrialCallback\n",
    "from optuna.study import MaxTrialsCallback\n",
    "from optuna.trial import TrialState\n",
    "par_dir = os.getcwd() #Parent Directory\n",
    "par_dir = Path(par_dir)\n",
    "sys.path.append(str(par_dir))\n",
    "from Functions import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f28f84541bf09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70a877fe27ee4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:14:33.815609700Z",
     "start_time": "2024-02-12T18:13:56.925544600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_folder = 'Result_3_1'\n",
    "#Read Main Data\n",
    "wcs_df = pd.read_csv(par_dir / 'Data' / 'UARK_WCS_AIS_Compiled_NewData.csv')\n",
    "cons_loc = pd.read_csv(par_dir / 'Data' / 'location_with_consistent_data_newdata.csv')\n",
    "wcs_df = pd.merge(wcs_df,cons_loc,how='left',on='sub_folder')\n",
    "\n",
    "terminal_type = wcs_df[['sub_folder','dominant_vessel_type']].drop_duplicates()\n",
    "comm_code = pd.read_csv(par_dir / 'Data' / 'WCSC_Commodity_CodesList.csv')\n",
    "#Keep unique SingleDigitCode and SingleDigitDescription from comm_code\n",
    "comm_code = comm_code[['SingleDigitCode','SingleDigitDescription']].drop_duplicates()\n",
    "\n",
    "prediction_combined_list = []\n",
    "prediction_combined_df = pd.DataFrame()\n",
    "#List of data files\n",
    "data_model_dir = {1:'UARK_WCS_AIS_Compiled_NewData_No_Aggregation.csv', 2:'UARK_WCS_AIS_Compiled_NewData_Mixed.csv', 3:'UARK_WCS_AIS_Compiled_NewData_Self-Propelled, Dry Cargo.csv',4:'UARK_WCS_AIS_Compiled_NewData_Self-Propelled, Tanker.csv',5:'UARK_WCS_AIS_Compiled_NewData_Tug_Tow.csv'}\n",
    "\n",
    "data_model_dir = {1:'UARK_WCS_AIS_Compiled_NewData_No_Aggregation.csv'}\n",
    "\n",
    "for i_filenumber in data_model_dir.keys():\n",
    "    #Read data\n",
    "    if os.getcwd() == '/home/USACE_Modeling':\n",
    "        data_num = int(sys.argv[1]) #For HPC\n",
    "    else:\n",
    "        data_num=i_filenumber\n",
    "    #Read Data\n",
    "    file_loc = par_dir / 'Data' / data_model_dir[data_num]\n",
    "    df = pd.read_csv(file_loc)\n",
    "    #Drop columns that start with 'dwell\n",
    "    df = df.drop(columns=[col for col in df.columns if col.lower().startswith('dwell_'.lower())])\n",
    "    \n",
    "    #Removing outliers\n",
    "    outlier_terminal = pd.read_csv(par_dir / 'Data' / 'outlier_terminals.csv')\n",
    "    #Remove records from outlier terminals whose sub_folder are in outlier_terminal['terminal']\n",
    "    df = df[~df['sub_folder'].isin(outlier_terminal['terminal'])]\n",
    "    \n",
    "    #Data processing\n",
    "    df['sin_quarter'] = np.sin(2*np.pi*(df['QuarterOfTheYear']-0)/4)\n",
    "    df['cos_quarter'] = np.cos(2*np.pi*(df['QuarterOfTheYear']-0)/4)\n",
    "    df = df.drop(columns=['QuarterOfTheYear'])\n",
    "    target = list(df.columns.values)\n",
    "    target = [idx for idx in target if idx.lower().startswith('C_'.lower())]\n",
    "    #Parameters\n",
    "    end_training = '2019Q1'\n",
    "    begin_validation = '2018Q1'\n",
    "    end_validation = '2020Q1'\n",
    "    begin_testing = '2019Q1'\n",
    "    end_testing = '2020Q4'\n",
    "    n_past = 4 #Look back period\n",
    "    n_future = 4 #Number of period to forecast each time\n",
    "    batch_size = 1 #For creating dataset\n",
    "    n_epochs = 1000\n",
    "    df = df.set_index(['quarter'])\n",
    "    df = df.loc[df.index <= end_testing]\n",
    "    #Reorder columns\n",
    "    df = df[target + [col for col in df.columns if col not in target]]\n",
    "    n_unknown_features = len(target)\n",
    "    n_features = len([x for x in list(df.columns.values) if x not in ['quarter','sub_folder']])\n",
    "    n_deterministic_features = n_features-n_unknown_features\n",
    "    train_columns_list = [x for x in list(df.columns.values) if x not in ['quarter','sub_folder']]\n",
    "        \n",
    "    ## Split and Scale\n",
    "    location_list = df[['sub_folder']].drop_duplicates()['sub_folder'].to_list()\n",
    "    train_df = []\n",
    "    val_df = []\n",
    "    test_df = []\n",
    "    for location in location_list:\n",
    "        temp_df = df[df['sub_folder']==location].copy()\n",
    "        temp_df = temp_df[train_columns_list]\n",
    "        temp_train = temp_df.loc[:end_training].iloc[:-1]\n",
    "        temp_val = temp_df.loc[begin_validation:end_validation].iloc[:-1]\n",
    "        temp_test = temp_df.loc[begin_testing:]\n",
    "        train_df.append(temp_train)\n",
    "        val_df.append(temp_val)\n",
    "        test_df.append(temp_test)\n",
    "    train_df = pd.concat(train_df)\n",
    "    test_df = pd.concat(test_df)\n",
    "    val_df = pd.concat(val_df)\n",
    "    true_labels = df[target+['sub_folder']][df[target+['sub_folder']].index.isin(test_df.index.unique()[n_past:].to_list())].copy()\n",
    "    true_lables_train = df[target+['sub_folder']][df[target+['sub_folder']].index.isin(train_df.index.unique()[n_past:].to_list())].copy()\n",
    "    true_labels_val = df[target+['sub_folder']][df[target+['sub_folder']].index.isin(val_df.index.unique()[n_past:].to_list())].copy()\n",
    "    # Scale the data\n",
    "    #Scale features\n",
    "    features = [x for x in list(train_df.columns.values) if x not in target]\n",
    "    #List of columns that start with stop or dwell\n",
    "    ais_features = [col for col in features if col.lower().startswith('stop'.lower()) or col.lower().startswith('dwell'.lower())]\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    other_features = [x for x in features if x not in ais_features]\n",
    "    #Scale other features\n",
    "    train_df[other_features] = feature_scaler.fit_transform(train_df[other_features])\n",
    "    val_df[other_features] = feature_scaler.transform(val_df[other_features])\n",
    "    test_df[other_features] = feature_scaler.transform(test_df[other_features])\n",
    "    #Scale AIS features\n",
    "    ais_min = train_df[ais_features].min().min()\n",
    "    ais_max = train_df[ais_features].max().max()\n",
    "    train_df[ais_features] = (train_df[ais_features] - ais_min)/(ais_max - ais_min)\n",
    "    val_df[ais_features] = (val_df[ais_features] - ais_min)/(ais_max - ais_min)\n",
    "    test_df[ais_features] = (test_df[ais_features] - ais_min)/(ais_max - ais_min)\n",
    "    #Scale target\n",
    "    train_min = train_df[target].min().min()\n",
    "    train_max = train_df[target].max().max()\n",
    "    train_df[target] = (train_df[target] - train_min)/(train_max - train_min)\n",
    "    val_df[target] = (val_df[target] - train_min)/(train_max - train_min)\n",
    "    test_df[target] = (test_df[target] - train_min)/(train_max - train_min)\n",
    "    #Create dataset\n",
    "    val_windowed = create_dataset(val_df, n_deterministic_features, n_past, n_future,batch_size, target)\n",
    "    training_windowed = create_dataset(train_df, n_deterministic_features, n_past, n_future,batch_size,target)\n",
    "    test_windowed = create_dataset(test_df, n_deterministic_features, n_past, n_future,batch_size, target)\n",
    "    \n",
    "    study_name = data_model_dir[data_num].split('.')[0]\n",
    "    #Load and compile model\n",
    "    #Find file name that starts with study_name and ends with .h5\n",
    "    model_path =par_dir / 'Outputs' / 'LSTM_Outputs' / prediction_folder\n",
    "    for filename in os.listdir(model_path):\n",
    "        if filename.startswith(study_name) and filename.endswith('.h5'):\n",
    "            model_name = filename\n",
    "    print(model_name)\n",
    "    \n",
    "    storage_path = \"Outputs/LSTM_Outputs/\" + prediction_folder + \"/LSTM_study.log\"\n",
    "    storage_url = optuna.storages.JournalStorage(optuna.storages.JournalFileStorage(storage_path),)\n",
    "    study = optuna.study.load_study(study_name = study_name, storage=storage_url)\n",
    "    study_temp_df = study.trials_dataframe()\n",
    "    #Get best trial\n",
    "    best_trial = study_temp_df[study_temp_df['value']==study_temp_df['value'].min()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = load_model(model_path / model_name, compile=False)\n",
    "    model.compile(loss=Huber(), optimizer=best_trial['params_optimizer'].to_list()[0], metrics='mse')\n",
    "    \n",
    "    temp_pred_list = []\n",
    "    temp_pred_df = pd.DataFrame()\n",
    "    \n",
    "    actuals, forecasts = [], []\n",
    "    for i, ((past, future), actual) in enumerate(test_windowed):\n",
    "        pred = model.predict((past, future), verbose=0, batch_size=1500)\n",
    "        actuals.append(actual.numpy().flatten())\n",
    "        forecasts.append(pred.flatten())\n",
    "    print('Predict completed')\n",
    "    #convert actuals to dataframe\n",
    "    actuals_df = pd.DataFrame(actuals)\n",
    "    #Convert to only one column starting from first row\n",
    "    actuals_df = actuals_df.stack().reset_index(drop=True)\n",
    "    actuals_df = pd.DataFrame(actuals_df, columns=['Actuals'])\n",
    "    \n",
    "    # #Convert forecasts to dataframe\n",
    "    forecasts_df = pd.DataFrame(forecasts)\n",
    "    #Convert to only one column starting from first row\n",
    "    forecasts_df = forecasts_df.stack().reset_index(drop=True)\n",
    "    forecasts_df = pd.DataFrame(forecasts_df, columns=['Predictions'])\n",
    "    \n",
    "    #Concatenate actuals and forecasts\n",
    "    actuals_predictions_df = pd.concat([actuals_df, forecasts_df], axis=1)\n",
    "    actuals_predictions_df\n",
    "    \n",
    "    #Rescale actuals and predictions using train_min and train_max\n",
    "    actuals_predictions_df['Actuals'] = (actuals_predictions_df['Actuals']*(train_max - train_min) + train_min)\n",
    "    actuals_predictions_df['Predictions'] = (actuals_predictions_df['Predictions']*(train_max - train_min) + train_min)\n",
    "    \n",
    "    #Set actuals to int\n",
    "    actuals_predictions_df['Actuals'] = actuals_predictions_df['Actuals'].round(0).astype(int)\n",
    "    #Create dataframe\n",
    "    temp_label_df = []\n",
    "    true_labels.reset_index(inplace=True)\n",
    "    #Iterate through each row of true_labels\n",
    "    for index, row in true_labels.iterrows():\n",
    "        quarter = row['quarter']\n",
    "        commodity = row[target]\n",
    "        sub_folder = row['sub_folder']\n",
    "        #Create dataframe using quarter, commodity, sub_folder\n",
    "        temp_df = pd.DataFrame({'quarter':quarter,'Commodity':target, 'Actuals':commodity,'sub_folder':sub_folder})\n",
    "        #Append to temp_label_df\n",
    "        temp_label_df.append(temp_df)\n",
    "    #Concatenate temp_label_df\n",
    "    temp_label_df = pd.concat(temp_label_df)\n",
    "    #Set actuals to int\n",
    "    temp_label_df['Actuals'] = temp_label_df['Actuals'].astype(int)\n",
    "    temp_label_df = temp_label_df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    #Merge actuals_predictions_df with temp_label_df on index\n",
    "    final_predictions_df = pd.merge(temp_label_df,actuals_predictions_df,how='left',left_index=True,right_index=True)\n",
    "    \n",
    "    #Raise error if Actuals_x and Actuals_y are not equal\n",
    "    if (final_predictions_df['Actuals_x'] != final_predictions_df['Actuals_y']).any():\n",
    "        raise ValueError('Actuals_x and Actuals_y are not equal')\n",
    "    #Drop Actuals_y\n",
    "    final_predictions_df = final_predictions_df.drop(columns=['Actuals_y'])\n",
    "    #Rename Actuals_x to Actuals\n",
    "    final_predictions_df = final_predictions_df.rename(columns={'Actuals_x':'Actuals'})\n",
    "    final_predictions_df['Model'] = 'LSTM'\n",
    "    final_predictions_df['Aggregation'] = study_name\n",
    "    prediction_combined_list.append(final_predictions_df)\n",
    "    \n",
    "prediction_combined_df = pd.concat(prediction_combined_list)\n",
    "#Removing outliers\n",
    "outlier_terminals_commodity = pd.read_csv(par_dir / 'Data' / 'outlier_terminals_commodity.csv')\n",
    "#Rename terminal to sub_folder and commodity to Commodity\n",
    "outlier_terminals_commodity = outlier_terminals_commodity.rename(columns={'terminal':'sub_folder','commodity':'Commodity'})\n",
    "#Remove records from prediction_combined_df whose sub_folder and commodity are in outlier_terminals_commodity in the same row\n",
    "prediction_combined_df = prediction_combined_df[~prediction_combined_df[['sub_folder','Commodity']].apply(tuple,1).isin(outlier_terminals_commodity[['sub_folder','Commodity']].apply(tuple,1))]\n",
    "#Keep string after last _ in Aggregation\n",
    "prediction_combined_df['Aggregation'] = prediction_combined_df['Aggregation'].str.split('NewData_').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665a00d4a8adfe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:14:33.835792200Z",
     "start_time": "2024-02-12T18:14:33.819793400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Keep row with first lowest value from best_trial\n",
    "best_hp = best_trial[best_trial['value']==best_trial['value'].min()].iloc[0]\n",
    "#Print best hyperparameters\n",
    "print('Best hyperparameters:')\n",
    "for key, value in best_hp.to_dict().items():\n",
    "    print('    {}: {}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e958a06302d50f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:14:37.581068Z",
     "start_time": "2024-02-12T18:14:37.535358300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_combined_df[(prediction_combined_df['Aggregation']=='No_Aggregation') & (prediction_combined_df['Predictions']<0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee78ff08ec86ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:14:46.513916600Z",
     "start_time": "2024-02-12T18:14:46.492500200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_combined_df[(prediction_combined_df['Aggregation']=='No_Aggregation') & (prediction_combined_df['sub_folder']=='PortElizabethBerthsNo52_98') & (prediction_combined_df['Commodity']=='C_2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb004403767a5752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T01:25:48.296920500Z",
     "start_time": "2023-12-18T01:25:48.255807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### prediction_combined_df.to_csv(par_dir / 'Outputs' / 'LSTM_Outputs' / prediction_folder /  'LSTM_Predictions.csv', mode='x', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe668475637130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:30:36.669024700Z",
     "start_time": "2024-02-12T18:30:36.594750300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8554bf17be324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T18:41:21.206975400Z",
     "start_time": "2024-02-12T18:41:21.127622600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(training_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616df9ccd1c69929",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
